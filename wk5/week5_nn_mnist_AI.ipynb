{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af843ea3",
   "metadata": {},
   "source": [
    "\n",
    "# Week 5 — Neural Networks (All-in-One Notebook)\n",
    "\n",
    "**Dataset:** MNIST (handwritten digits 28×28)\n",
    "\n",
    "This single notebook consolidates the whole week plan (Days 29–35):\n",
    "- **Day 29:** 2-Layer NN from scratch (NumPy) – forward/backprop + gradient check  \n",
    "- **Day 30:** Activations + Testing (ReLU/Sigmoid), debugging, simple visualization  \n",
    "- **Day 31:** PyTorch MLP – build & quick train  \n",
    "- **Day 32:** Train on MNIST – baseline results  \n",
    "- **Day 33:** Optimizers – compare SGD vs Adam (loss curves)  \n",
    "- **Day 34:** TensorBoard – log and visualize training  \n",
    "- **Day 35:** Weekly Summary – cleanup & README\n",
    "\n",
    "> Tip: Run Runtime → Restart & Run All to ensure everything works from a cold start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04c6aa",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment & Requirements (Optional)\n",
    "If your environment does not already have the required libraries, install them first (uncomment to run):\n",
    "\n",
    "```bash\n",
    "# !pip install numpy matplotlib torch torchvision tensorboard scikit-learn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d25fa0",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup & Utilities\n",
    "Utilities shared across the week: seeding, device selection, plotting helpers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def plot_series(y_values, title=\"Metric over epochs\", xlabel=\"Epoch\", ylabel=\"Value\"):\n",
    "    plt.figure()\n",
    "    plt.plot(y_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def accuracy(pred_logits, targets):\n",
    "    preds = pred_logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928707a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Day 29) 2-Layer Neural Network (NumPy): Forward, Backprop, Gradient Check\n",
    "We'll implement a simple **2-layer MLP** (Input → Hidden → Output) with Softmax cross-entropy loss.\n",
    "\n",
    "**Plan**\n",
    "1. Build synthetic toy data (small, 2D) for easy debugging and visualization.\n",
    "2. Implement forward pass.\n",
    "3. Implement backward pass (manual gradients).\n",
    "4. Add **gradient checking** using finite differences on a tiny batch to validate backprop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic toy data (2D) — two Gaussian clusters\n",
    "def make_toy_data(n_per_class=100):\n",
    "    mean0, mean1 = np.array([-1.0, -1.0]), np.array([1.0, 1.0])\n",
    "    cov = np.array([[0.3, 0.0],[0.0, 0.3]])\n",
    "    X0 = np.random.multivariate_normal(mean0, cov, size=n_per_class)\n",
    "    X1 = np.random.multivariate_normal(mean1, cov, size=n_per_class)\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.concatenate([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\n",
    "    return X, y\n",
    "\n",
    "X_toy, y_toy = make_toy_data(80)\n",
    "X_toy.shape, y_toy.shape, np.bincount(y_toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1135a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2-layer MLP implemented in NumPy\n",
    "class TwoLayerMLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation=\"relu\", weight_scale=0.01, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = weight_scale * rng.standard_normal((input_dim, hidden_dim))\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = weight_scale * rng.standard_normal((hidden_dim, output_dim))\n",
    "        self.b2 = np.zeros((1, output_dim))\n",
    "        self.activation = activation\n",
    "\n",
    "    def _act(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation\")  \n",
    "\n",
    "    def _act_grad(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return (z > 0).astype(z.dtype)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            s = 1.0 / (1.0 + np.exp(-z))\n",
    "            return s * (1 - s)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = X @ self.W1 + self.b1      # (N, H)\n",
    "        a1 = self._act(z1)              # (N, H)\n",
    "        scores = a1 @ self.W2 + self.b2 # (N, C)\n",
    "        cache = (X, z1, a1, scores)\n",
    "        return scores, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(logits):\n",
    "        logits = logits - logits.max(axis=1, keepdims=True)\n",
    "        e = np.exp(logits)\n",
    "        return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(probs, y):\n",
    "        N = y.shape[0]\n",
    "        return -np.log(probs[np.arange(N), y] + 1e-12).mean()\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        scores, cache = self.forward(X)\n",
    "        probs = self.softmax(scores)\n",
    "        L = self.cross_entropy(probs, y)\n",
    "        return L, probs, cache\n",
    "\n",
    "    def backward(self, probs, cache, y):\n",
    "        X, z1, a1, scores = cache\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Gradient wrt scores\n",
    "        dscores = probs.copy()\n",
    "        dscores[np.arange(N), y] -= 1.0\n",
    "        dscores /= N\n",
    "\n",
    "        # Backprop into W2, b2, a1\n",
    "        dW2 = a1.T @ dscores              # (H, C)\n",
    "        db2 = dscores.sum(axis=0, keepdims=True)  # (1, C)\n",
    "        da1 = dscores @ self.W2.T         # (N, H)\n",
    "\n",
    "        # Backprop through activation\n",
    "        dz1 = da1 * self._act_grad(z1)    # (N, H)\n",
    "\n",
    "        # Backprop into W1, b1\n",
    "        dW1 = X.T @ dz1                   # (D, H)\n",
    "        db1 = dz1.sum(axis=0, keepdims=True)  # (1, H)\n",
    "        return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient check (finite differences) for a tiny batch\n",
    "def grad_check(model, X, y, eps=1e-5):\n",
    "    # Forward & backprop to get analytic grads\n",
    "    L, probs, cache = model.loss(X, y)\n",
    "    dW1, db1, dW2, db2 = model.backward(probs, cache, y)\n",
    "\n",
    "    def rel_error(a, b):\n",
    "        return np.abs(a - b).max() / (np.maximum(1e-8, np.abs(a) + np.abs(b))).max()\n",
    "\n",
    "    # Check a few random elements per parameter\n",
    "    rng = np.random.default_rng(0)\n",
    "    checks = []\n",
    "\n",
    "    # Helper to compute loss with current params\n",
    "    def loss_with_params():\n",
    "        L, _, _ = model.loss(X, y)\n",
    "        return L\n",
    "\n",
    "    for name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "        param = getattr(model, name)\n",
    "        grad = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}[name]\n",
    "        flat_idx = rng.choice(param.size, size=min(10, param.size), replace=False)\n",
    "        idxs = np.array(np.unravel_index(flat_idx, param.shape)).T\n",
    "\n",
    "        for (i, j) in idxs:\n",
    "            old_val = param[i, j] if param.ndim == 2 else param[0, j]\n",
    "            # positive perturb\n",
    "            if param.ndim == 2:\n",
    "                param[i, j] = old_val + eps\n",
    "            else:\n",
    "                param[0, j] = old_val + eps\n",
    "            L_pos = loss_with_params()\n",
    "            # negative perturb\n",
    "            if param.ndim == 2:\n",
    "                param[i, j] = old_val - eps\n",
    "            else:\n",
    "                param[0, j] = old_val - eps\n",
    "            L_neg = loss_with_params()\n",
    "            # restore\n",
    "            if param.ndim == 2:\n",
    "                param[i, j] = old_val\n",
    "            else:\n",
    "                param[0, j] = old_val\n",
    "            num_grad = (L_pos - L_neg) / (2 * eps)\n",
    "            ana_grad = grad[i, j] if grad.ndim == 2 else grad[0, j]\n",
    "            checks.append(rel_error(num_grad, ana_grad))\n",
    "    return max(checks), np.mean(checks)\n",
    "\n",
    "# run check on a tiny subset\n",
    "X_small, y_small = X_toy[:5], y_toy[:5]\n",
    "model_np = TwoLayerMLP(input_dim=2, hidden_dim=5, output_dim=2, activation=\"relu\", weight_scale=1e-2)\n",
    "max_err, mean_err = grad_check(model_np, X_small, y_small)\n",
    "print({\"max_rel_error\": max_err, \"mean_rel_error\": mean_err})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91669e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the NumPy model on toy data (for sanity)\n",
    "def train_numpy(model, X, y, lr=1.0, epochs=200):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        L, probs, cache = model.loss(X, y)\n",
    "        dW1, db1, dW2, db2 = model.backward(probs, cache, y)\n",
    "        # SGD step\n",
    "        model.W1 -= lr * dW1\n",
    "        model.b1 -= lr * db1\n",
    "        model.W2 -= lr * dW2\n",
    "        model.b2 -= lr * db2\n",
    "        losses.append(L)\n",
    "        if (epoch+1) % (epochs//4) == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {L:.4f}\")\n",
    "    return losses\n",
    "\n",
    "model_np = TwoLayerMLP(input_dim=2, hidden_dim=8, output_dim=2, activation=\"relu\", weight_scale=1e-1)\n",
    "loss_curve = train_numpy(model_np, X_toy, y_toy, lr=0.5, epochs=200)\n",
    "\n",
    "# Plot loss (one chart, default style, no explicit color)\n",
    "plot_series(loss_curve, title=\"NumPy 2-Layer MLP - Training Loss (Toy Data)\", xlabel=\"Epoch\", ylabel=\"Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20be7b2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Day 30) Activations + Testing\n",
    "Try **ReLU** and **Sigmoid**, compare training loss on the toy dataset. This helps confirm your backprop math.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_curves = {}\n",
    "for act in [\"relu\", \"sigmoid\"]:\n",
    "    m = TwoLayerMLP(2, 16, 2, activation=act, weight_scale=0.1, seed=SEED)\n",
    "    lc = train_numpy(m, X_toy, y_toy, lr=0.3, epochs=150)\n",
    "    loss_curves[act] = lc\n",
    "\n",
    "# Plot curves separately (follow rule: one chart per figure)\n",
    "plot_series(loss_curves[\"relu\"], title=\"Activation: ReLU (Toy Data)\", xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "plot_series(loss_curves[\"sigmoid\"], title=\"Activation: Sigmoid (Toy Data)\", xlabel=\"Epoch\", ylabel=\"Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cbb49",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Day 31–32) PyTorch MLP for MNIST\n",
    "We now switch to **MNIST** and build a simple feedforward network in PyTorch, then train & evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72592ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),           # [0, 1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # standard MNIST stats\n",
    "])\n",
    "\n",
    "root = \"./data\"\n",
    "train_dataset = datasets.MNIST(root=root, train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "len(train_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32efbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/val split from training set\n",
    "val_ratio = 0.1\n",
    "n_val = int(len(train_dataset) * val_ratio)\n",
    "n_train = len(train_dataset) - n_val\n",
    "train_ds, val_ds = random_split(train_dataset, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "n_train, n_val, len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MNISTMLP(nn.Module):\n",
    "    def __init__(self, in_dim=28*28, hidden=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MNISTMLP().to(DEVICE)\n",
    "sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_count += xb.size(0)\n",
    "    return total_loss / total_count, total_correct / total_count\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_count += xb.size(0)\n",
    "    return total_loss / total_count, total_correct / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline training\n",
    "EPOCHS = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    va_loss, va_acc = evaluate(model, val_loader, criterion)\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    val_accs.append(va_acc)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss {tr_loss:.4f} | Val Loss {va_loss:.4f} | Val Acc {va_acc:.4f}\")\n",
    "\n",
    "plot_series(train_losses, title=\"MNIST MLP — Training Loss\", xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "plot_series(val_losses, title=\"MNIST MLP — Validation Loss\", xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "plot_series(val_accs, title=\"MNIST MLP — Validation Accuracy\", xlabel=\"Epoch\", ylabel=\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test set evaluation\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print({\"test_loss\": round(test_loss, 4), \"test_acc\": round(test_acc, 4)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "save_path = Path(\"./mnist_mlp.pth\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9b77a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Day 33) Optimizers — SGD vs Adam\n",
    "We train **two identical models** with different optimizers and plot the loss/accuracy curves for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_optimizer(optimizer_name=\"SGD\", epochs=5, lr=0.001):\n",
    "    m = MNISTMLP().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optimizer_name.lower() == \"sgd\":\n",
    "        opt = torch.optim.SGD(m.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == \"adam\":\n",
    "        opt = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    tr_losses, va_losses, va_accs = [], [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, _ = train_one_epoch(m, train_loader, opt, criterion)\n",
    "        va_loss, va_acc = evaluate(m, val_loader, criterion)\n",
    "        tr_losses.append(tr_loss); va_losses.append(va_loss); va_accs.append(va_acc)\n",
    "    return tr_losses, va_losses, va_accs\n",
    "\n",
    "sgd_tr, sgd_va, sgd_acc = train_model_with_optimizer(\"SGD\", epochs=5, lr=0.1)\n",
    "adam_tr, adam_va, adam_acc = train_model_with_optimizer(\"Adam\", epochs=5, lr=0.001)\n",
    "\n",
    "# Plot separately per rule (single plot per figure)\n",
    "plot_series(sgd_va, title=\"Validation Loss — SGD\", xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "plot_series(adam_va, title=\"Validation Loss — Adam\", xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "plot_series(sgd_acc, title=\"Validation Accuracy — SGD\", xlabel=\"Epoch\", ylabel=\"Accuracy\")\n",
    "plot_series(adam_acc, title=\"Validation Accuracy — Adam\", xlabel=\"Epoch\", ylabel=\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520397d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Day 34) TensorBoard Logging\n",
    "We'll log training/validation metrics to **TensorBoard** using `SummaryWriter`.\n",
    "\n",
    "**How to launch locally (in a terminal):**\n",
    "```bash\n",
    "tensorboard --logdir runs --port 6006\n",
    "# then open http://localhost:6006 in your browser\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"runs/mnist_mlp\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "model_tb = MNISTMLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_tb.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS_TB = 3\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS_TB+1):\n",
    "    model_tb.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_tb(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log training loss\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    # Log validation metrics per epoch\n",
    "    val_loss, val_acc = evaluate(model_tb, val_loader, criterion)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "\n",
    "# Optionally, add the computational graph\n",
    "example_input = torch.randn(1, 1, 28, 28).to(DEVICE)\n",
    "writer.add_graph(model_tb, example_input)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "print(f\"TensorBoard logs written to: {log_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe908c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Day 35) Weekly Summary & README\n",
    "\n",
    "Use this section to summarize your results and lessons. Fill in the placeholders after running the experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d10afe",
   "metadata": {},
   "source": [
    "\n",
    "### Results Snapshot (Fill me in)\n",
    "- **MNIST Test Accuracy (baseline MLP):** `__%`\n",
    "- **SGD vs Adam (val acc after 5 epochs):** `SGD: __%`, `Adam: __%`\n",
    "- **Key learnings:**\n",
    "  - Backprop math validated with gradient check (max relative error ~1e-__)\n",
    "  - ReLU vs Sigmoid behavior on toy data\n",
    "  - Optimizer impact on convergence speed and stability\n",
    "  - TensorBoard helped inspect loss & accuracy\n",
    "\n",
    "### Next ideas\n",
    "- Try BatchNorm / Dropout\n",
    "- CNN (LeNet-like) on MNIST for higher accuracy\n",
    "- Learning rate schedules / OneCycle\n",
    "- Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Notebook generated on: 2025-11-06T17:04:25.621227Z\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
